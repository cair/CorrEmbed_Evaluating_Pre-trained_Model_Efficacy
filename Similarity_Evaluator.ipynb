{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import Database_connector\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from Database_Settings import DB_SETTINGS\n",
    "import Database_connector\n",
    "from mysql.connector import (connection)\n",
    "from PIL import Image\n",
    "\n",
    "PICKLED_DATA_FOLDER = \"Pickled_Data/\"\n",
    "PICKLED_PICTURES_FILE = \"pictures_embeddings_resnet152_df.pkl\"\n",
    "PICKLED_OUTFIRS_FILE = \"active_outfits_df.pkl\"\n",
    "\n",
    "PICTURES_DROP_COLUMNS = [\"contentType\", \"status\", \"displayOrder\", \"sourceURL\", \"embeddings\"]\n",
    "#PICTURES_DROP_COLUMNS = []\n",
    "OUTFITS_DROP_COLUMNS = ['owner', 'name', 'brand', 'isPublic', 'isDeleted', 'meta.validFrom', 'meta.validTo', \"Outfit_size\"]\n",
    "\n",
    "\n",
    "def get_active_outfits():\n",
    "    cnx = connection.MySQLConnection(**DB_SETTINGS)\n",
    "    db_connection = Database_connector.Db_Connection()\n",
    "    cursor = cnx.cursor(dictionary=True)\n",
    "    #tag_query = \"SELECT * FROM Outfits WHERE (`Outfits`.`isPublic` = TRUE AND `Outfits`.`isDeleted` = FALSE AND `Outfits`.`meta.validTo` >= '9999-01-01 00:00:00')\"\n",
    "    tag_query = \"SELECT `Outfits`.`id` AS `id`, `Outfits`.`owner` AS `owner`, `Outfits`.`name` AS `name`, `Outfits`.`brand` AS `brand`, `Outfits`.`isPublic` AS `isPublic`, `Outfits`.`isDeleted` AS `isDeleted`, `Outfits`.`timeCreated` AS `timeCreated`, `Outfits`.`timeUpdated` AS `timeUpdated`, `Outfits`.`pricePerWeek` AS `pricePerWeek`, `Outfits`.`pricePerMonth` AS `pricePerMonth`, `Outfits`.`type` AS `type`, `Outfits`.`keywords` AS `keywords`, `Outfits`.`retailPrice` AS `retailPrice`, `Outfits`.`meta.validFrom` AS `meta.validFrom`, `Outfits`.`meta.validTo` AS `meta.validTo` FROM `Outfits` WHERE (`Outfits`.`isPublic` = TRUE AND `Outfits`.`isDeleted` = FALSE AND `Outfits`.`meta.validTo` >= '9999-01-01 00:00:00')\"\n",
    "    cursor.execute(tag_query)\n",
    "    outfit_results = cursor.fetchall()\n",
    "\n",
    "    outfits_df_db = pd.DataFrame([list(order_dict.values()) for order_dict in outfit_results], columns=list(outfit_results[0].keys()))\n",
    "    return outfits_df_db\n",
    "\n",
    "def get_pictures_df(path):\n",
    "    pictures_df = pd.read_pickle(path)\n",
    "    #pictures_df = pictures_df.drop(columns=PICTURES_DROP_COLUMNS)\n",
    "    pictures_df = pictures_df.drop(columns=pictures_df.columns[2:])\n",
    "    return pictures_df\n",
    "\n",
    "def get_outfits_df(path):\n",
    "    outfits_df = pd.read_pickle(path)\n",
    "    outfits_df = outfits_df.drop(columns=OUTFITS_DROP_COLUMNS)\n",
    "    return outfits_df\n",
    "\n",
    "def prepare_data(pictures_df_path = PICKLED_DATA_FOLDER + PICKLED_PICTURES_FILE, outfits_df_path = PICKLED_DATA_FOLDER + PICKLED_OUTFIRS_FILE):\n",
    "    pictures_df = get_pictures_df(pictures_df_path)\n",
    "    outfits_df = get_outfits_df(outfits_df_path)\n",
    "    return pictures_df, outfits_df\n",
    "\n",
    "def picture_exists(picture_id, pictures_dir_path):\n",
    "    return os.path.isfile(pictures_dir_path + os.sep + picture_id + \".jpg\")\n",
    "\n",
    "def find_missing_pictures(pictures_df, pictures_dir_path):\n",
    "    pictures_df[\"file_exists\"] = pictures_df[\"id\"].apply(lambda x : picture_exists(x, pictures_dir_path))\n",
    "    return pictures_df[\"file_exists\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "def load_resnet152_model():\n",
    "    model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet152', weights=torchvision.models.ResNet152_Weights.IMAGENET1K_V1)\n",
    "    embedding_model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "    embedding_model.eval()\n",
    "    return embedding_model\n",
    "\n",
    "def load_resnet101_model():\n",
    "    model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet101', weights=torchvision.models.ResNet101_Weights.IMAGENET1K_V1)\n",
    "    embedding_model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "    embedding_model.eval()\n",
    "    return embedding_model\n",
    "\n",
    "def load_resnet50_model():\n",
    "    model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet50', weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "    embedding_model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "    embedding_model.eval()\n",
    "    return embedding_model\n",
    "\n",
    "def load_resnet34_model():\n",
    "    model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet34', weights=torchvision.models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "    embedding_model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "    embedding_model.eval()\n",
    "    return embedding_model\n",
    "\n",
    "def load_resnet18_model():\n",
    "    model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    embedding_model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "    embedding_model.eval()\n",
    "    return embedding_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>weight</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sleeve</td>\n",
       "      <td>0.496527</td>\n",
       "      <td>46.290323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Waist</td>\n",
       "      <td>0.428134</td>\n",
       "      <td>42.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Material</td>\n",
       "      <td>0.268592</td>\n",
       "      <td>47.159574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Length</td>\n",
       "      <td>0.542224</td>\n",
       "      <td>45.185185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pattern</td>\n",
       "      <td>0.500707</td>\n",
       "      <td>43.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Brand</td>\n",
       "      <td>0.198210</td>\n",
       "      <td>51.013514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fit</td>\n",
       "      <td>0.454452</td>\n",
       "      <td>47.192308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Neckline</td>\n",
       "      <td>0.431552</td>\n",
       "      <td>43.078947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Category</td>\n",
       "      <td>0.435782</td>\n",
       "      <td>50.550725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Size</td>\n",
       "      <td>0.479231</td>\n",
       "      <td>47.977778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>No category</td>\n",
       "      <td>0.309975</td>\n",
       "      <td>48.286885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Occassion</td>\n",
       "      <td>0.324405</td>\n",
       "      <td>48.544776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Color</td>\n",
       "      <td>0.248423</td>\n",
       "      <td>45.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Embellishment</td>\n",
       "      <td>0.546830</td>\n",
       "      <td>32.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Occasion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         category    weight        sum\n",
       "0          Sleeve  0.496527  46.290323\n",
       "1           Waist  0.428134  42.142857\n",
       "2        Material  0.268592  47.159574\n",
       "3          Length  0.542224  45.185185\n",
       "4         Pattern  0.500707  43.588235\n",
       "5           Brand  0.198210  51.013514\n",
       "6             Fit  0.454452  47.192308\n",
       "7        Neckline  0.431552  43.078947\n",
       "8        Category  0.435782  50.550725\n",
       "9            Size  0.479231  47.977778\n",
       "10    No category  0.309975  48.286885\n",
       "11      Occassion  0.324405  48.544776\n",
       "12          Color  0.248423  45.714286\n",
       "13  Embellishment  0.546830  32.272727\n",
       "14       Occasion       NaN        NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_weights = pd.read_pickle(\"Pickled_Data/weights_df.pkl\")\n",
    "tag_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<enum 'ResNet50_Weights'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models import get_model_weights\n",
    "\n",
    "# print model weight names\n",
    "get_model_weights(\"Resnet152\")\n",
    "torchvision.models.ResNet18_Weights.IMAGENET1K_V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>owner</th>\n",
       "      <th>contentType</th>\n",
       "      <th>status</th>\n",
       "      <th>displayOrder</th>\n",
       "      <th>sourceURL</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>picture.00058abb53434872ae9bb4270ae21f8e</td>\n",
       "      <td>outfit.98f32aaf08bc4ff09c44e6e11e9199bc</td>\n",
       "      <td>image/jpeg</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>2</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/0424/9927/69...</td>\n",
       "      <td>[tensor(0.2495), tensor(1.2492), tensor(0.), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>picture.000cf715019a4a02aaf0dc479212195a</td>\n",
       "      <td>outfit.f11bdc25d87946f8b831b608bc8fb574</td>\n",
       "      <td>image/jpeg</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>1</td>\n",
       "      <td>https://storage.googleapis.com/batch-uploads-p...</td>\n",
       "      <td>[tensor(0.2230), tensor(1.3119), tensor(0.4345...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>picture.0010c2e161154d6893734981d5455e76</td>\n",
       "      <td>outfit.9387d05b47f906c5</td>\n",
       "      <td>image/jpeg</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[tensor(0.1586), tensor(0.8255), tensor(0.9512...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>picture.00135ce8b1a04c5daa60cf7bdd99bcd5</td>\n",
       "      <td>outfit.12ae28ab0dc5494c98a1de2f8ce04b79</td>\n",
       "      <td>image/jpeg</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>1</td>\n",
       "      <td>https://storage.googleapis.com/batch-uploads-p...</td>\n",
       "      <td>[tensor(0.2306), tensor(0.3835), tensor(0.1205...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>picture.001a58ea68da426384567b8cccc0c8a6</td>\n",
       "      <td>outfit.e989b8cb4a814d97b642e1cb326f47e6</td>\n",
       "      <td>image/png</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[tensor(0.0074), tensor(2.0541), tensor(0.5028...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18055</th>\n",
       "      <td>picture.ffe927a5dc06491fb73958a896ac2b19</td>\n",
       "      <td>outfit.0263a299144e4c89b68dd66990e43d65</td>\n",
       "      <td>image/jpeg</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>1</td>\n",
       "      <td>https://storage.googleapis.com/batch-uploads-p...</td>\n",
       "      <td>[tensor(0.1538), tensor(1.2303), tensor(0.1667...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18056</th>\n",
       "      <td>picture.fff4edd997914149927603fb0a9a3009</td>\n",
       "      <td>outfit.4779ad6771a746408fb5d492ae345d18</td>\n",
       "      <td>image/jpeg</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>2</td>\n",
       "      <td>https://storage.googleapis.com/batch-uploads-p...</td>\n",
       "      <td>[tensor(0.4513), tensor(1.9715), tensor(0.1843...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18057</th>\n",
       "      <td>picture.fff842f598dd4c3d9c80d2c1e00c4a26</td>\n",
       "      <td>outfit.c9ba76ae33c44aa5a06b23a4e4229fb1</td>\n",
       "      <td>image/jpeg</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[tensor(0.0028), tensor(0.5193), tensor(0.1181...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18058</th>\n",
       "      <td>picture.fffa8417a83642e0aba3c8187e80a3b9</td>\n",
       "      <td>outfit.225b573681f9446a8b5dc126add381b8</td>\n",
       "      <td>image/jpeg</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>2</td>\n",
       "      <td>https://storage.googleapis.com/batch-uploads-p...</td>\n",
       "      <td>[tensor(0.4813), tensor(1.8617), tensor(0.0046...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18059</th>\n",
       "      <td>picture.fffd49c19adb41eca997f2446c809ba0</td>\n",
       "      <td>outfit.8683d74742d24a1fa0078771b0c8e469</td>\n",
       "      <td>image/jpeg</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>3</td>\n",
       "      <td>https://storage.googleapis.com/batch-uploads-p...</td>\n",
       "      <td>[tensor(0.0179), tensor(1.5484), tensor(0.2937...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18060 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             id  \\\n",
       "0      picture.00058abb53434872ae9bb4270ae21f8e   \n",
       "1      picture.000cf715019a4a02aaf0dc479212195a   \n",
       "2      picture.0010c2e161154d6893734981d5455e76   \n",
       "3      picture.00135ce8b1a04c5daa60cf7bdd99bcd5   \n",
       "4      picture.001a58ea68da426384567b8cccc0c8a6   \n",
       "...                                         ...   \n",
       "18055  picture.ffe927a5dc06491fb73958a896ac2b19   \n",
       "18056  picture.fff4edd997914149927603fb0a9a3009   \n",
       "18057  picture.fff842f598dd4c3d9c80d2c1e00c4a26   \n",
       "18058  picture.fffa8417a83642e0aba3c8187e80a3b9   \n",
       "18059  picture.fffd49c19adb41eca997f2446c809ba0   \n",
       "\n",
       "                                         owner contentType  status  \\\n",
       "0      outfit.98f32aaf08bc4ff09c44e6e11e9199bc  image/jpeg  ACTIVE   \n",
       "1      outfit.f11bdc25d87946f8b831b608bc8fb574  image/jpeg  ACTIVE   \n",
       "2                      outfit.9387d05b47f906c5  image/jpeg  ACTIVE   \n",
       "3      outfit.12ae28ab0dc5494c98a1de2f8ce04b79  image/jpeg  ACTIVE   \n",
       "4      outfit.e989b8cb4a814d97b642e1cb326f47e6   image/png  ACTIVE   \n",
       "...                                        ...         ...     ...   \n",
       "18055  outfit.0263a299144e4c89b68dd66990e43d65  image/jpeg  ACTIVE   \n",
       "18056  outfit.4779ad6771a746408fb5d492ae345d18  image/jpeg  ACTIVE   \n",
       "18057  outfit.c9ba76ae33c44aa5a06b23a4e4229fb1  image/jpeg  ACTIVE   \n",
       "18058  outfit.225b573681f9446a8b5dc126add381b8  image/jpeg  ACTIVE   \n",
       "18059  outfit.8683d74742d24a1fa0078771b0c8e469  image/jpeg  ACTIVE   \n",
       "\n",
       "       displayOrder                                          sourceURL  \\\n",
       "0                 2  https://cdn.shopify.com/s/files/1/0424/9927/69...   \n",
       "1                 1  https://storage.googleapis.com/batch-uploads-p...   \n",
       "2                 0                                               None   \n",
       "3                 1  https://storage.googleapis.com/batch-uploads-p...   \n",
       "4                 0                                               None   \n",
       "...             ...                                                ...   \n",
       "18055             1  https://storage.googleapis.com/batch-uploads-p...   \n",
       "18056             2  https://storage.googleapis.com/batch-uploads-p...   \n",
       "18057             0                                               None   \n",
       "18058             2  https://storage.googleapis.com/batch-uploads-p...   \n",
       "18059             3  https://storage.googleapis.com/batch-uploads-p...   \n",
       "\n",
       "                                              embeddings  \n",
       "0      [tensor(0.2495), tensor(1.2492), tensor(0.), t...  \n",
       "1      [tensor(0.2230), tensor(1.3119), tensor(0.4345...  \n",
       "2      [tensor(0.1586), tensor(0.8255), tensor(0.9512...  \n",
       "3      [tensor(0.2306), tensor(0.3835), tensor(0.1205...  \n",
       "4      [tensor(0.0074), tensor(2.0541), tensor(0.5028...  \n",
       "...                                                  ...  \n",
       "18055  [tensor(0.1538), tensor(1.2303), tensor(0.1667...  \n",
       "18056  [tensor(0.4513), tensor(1.9715), tensor(0.1843...  \n",
       "18057  [tensor(0.0028), tensor(0.5193), tensor(0.1181...  \n",
       "18058  [tensor(0.4813), tensor(1.8617), tensor(0.0046...  \n",
       "18059  [tensor(0.0179), tensor(1.5484), tensor(0.2937...  \n",
       "\n",
       "[18060 rows x 7 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "original_pickle_path = \"Pickled_Data/pictures_embeddings_resnet152_df.pkl\"\n",
    "pd.read_pickle(original_pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>owner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>picture.00058abb53434872ae9bb4270ae21f8e</td>\n",
       "      <td>outfit.98f32aaf08bc4ff09c44e6e11e9199bc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>picture.000cf715019a4a02aaf0dc479212195a</td>\n",
       "      <td>outfit.f11bdc25d87946f8b831b608bc8fb574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>picture.0010c2e161154d6893734981d5455e76</td>\n",
       "      <td>outfit.9387d05b47f906c5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>picture.00135ce8b1a04c5daa60cf7bdd99bcd5</td>\n",
       "      <td>outfit.12ae28ab0dc5494c98a1de2f8ce04b79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>picture.001a58ea68da426384567b8cccc0c8a6</td>\n",
       "      <td>outfit.e989b8cb4a814d97b642e1cb326f47e6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  \\\n",
       "0  picture.00058abb53434872ae9bb4270ae21f8e   \n",
       "1  picture.000cf715019a4a02aaf0dc479212195a   \n",
       "2  picture.0010c2e161154d6893734981d5455e76   \n",
       "3  picture.00135ce8b1a04c5daa60cf7bdd99bcd5   \n",
       "4  picture.001a58ea68da426384567b8cccc0c8a6   \n",
       "\n",
       "                                     owner  \n",
       "0  outfit.98f32aaf08bc4ff09c44e6e11e9199bc  \n",
       "1  outfit.f11bdc25d87946f8b831b608bc8fb574  \n",
       "2                  outfit.9387d05b47f906c5  \n",
       "3  outfit.12ae28ab0dc5494c98a1de2f8ce04b79  \n",
       "4  outfit.e989b8cb4a814d97b642e1cb326f47e6  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pictures_df, outfits_df = prepare_data(pictures_df_path=\"../FREja_dataset_processing/pictures_embeddings_resnet152_df.pkl\"\n",
    "                                       , outfits_df_path=\"../FREja_dataset_processing/active_outfits_df.pkl\")\n",
    "pictures_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_SAVE_DIR = \"Pickled_Data/Embeddings/\"\n",
    "PICTURES_DIR_PATH = \"../FREja_dataset_processing/Filtered_pictures/\"\n",
    "DF_SPLITS = 200\n",
    "RUN_CONFIGS = [\n",
    "    {\n",
    "        \"model\" : load_resnet152_model,\n",
    "        \"save_file_name\" : \"resnet152_v1Embeddings_df.pkl\"\n",
    "    },\n",
    "    {\n",
    "        \"model\" : load_resnet101_model,\n",
    "        \"save_file_name\" : \"resnet101_v1Embeddings_df.pkl\"\n",
    "    },\n",
    "    {\n",
    "        \"model\" : load_resnet50_model,\n",
    "        \"save_file_name\" : \"resnet50_v1Embeddings_df.pkl\"\n",
    "    },\n",
    "    {\n",
    "        \"model\" : load_resnet34_model,\n",
    "        \"save_file_name\" : \"resnet34_v1Embeddings_df.pkl\"\n",
    "    },\n",
    "    {\n",
    "        \"model\" : load_resnet18_model,\n",
    "        \"save_file_name\" : \"resnet18_v1Embeddings_df.pkl\"\n",
    "    }\n",
    "]\n",
    "\n",
    "if not os.path.isdir(EMBEDDINGS_SAVE_DIR):\n",
    "    os.mkdir(EMBEDDINGS_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    18060\n",
       "Name: file_exists, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_missing_pictures(pictures_df, PICTURES_DIR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import copy\n",
    "from IPython.display import display\n",
    "import ipywidgets\n",
    "from tqdm import tqdm\n",
    "\n",
    "#file_path = \"C:\\\\Datasets\\\\FJONG_image_dataset\\\\FWSS PHOTOS 20-6-22\\\\Ain't No Mountain - 2 copy.jpg\"\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def load_image(image_id):\n",
    "    file_name = image_id + \".jpg\"\n",
    "    file_path = PICTURES_DIR_PATH + file_name\n",
    "    image = Image.open(file_path)\n",
    "    if image.mode != \"RGB\":\n",
    "        print(f\"Found image with mode: {image.mode}, id: {image_id}\")\n",
    "        image = image.convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "def image_to_embedding(preprocess_transform, model_e, image_id, d_widget, index_num):\n",
    "    d_widget.value = f\"Converting file number: {index_num}\"\n",
    "    pil_image = load_image(image_id)\n",
    "    input_tensor = preprocess(pil_image).unsqueeze(0)\n",
    "    if torch.cuda.is_available():\n",
    "        input_tensor = input_tensor.to('cuda')\n",
    "\n",
    "    embedding = embedding_model(input_tensor)\n",
    "    return embedding[0].squeeze().cpu()\n",
    "\n",
    "def get_df_embeddings(pictures_df, embedding_model, preprocess):\n",
    "    display_out = ipywidgets.HTML()\n",
    "    display(display_out)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        embedding_model.to('cuda')\n",
    "\n",
    "    split_dfs = np.array_split(pictures_df, DF_SPLITS)\n",
    "    with torch.no_grad():\n",
    "        for df_split in tqdm(split_dfs):\n",
    "            df_split[\"embeddings\"] = df_split.apply(lambda row: image_to_embedding(preprocess, embedding_model, row[\"id\"], display_out, row.name), axis=1)\n",
    "    embedding_df = pd.concat(split_dfs)\n",
    "    return embedding_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in RUN_CONFIGS:\n",
    "    embedding_model = config[\"model\"]()\n",
    "    to_embeddings_df = pictures_df.copy()\n",
    "    embedding_df = get_df_embeddings(to_embeddings_df, embedding_model, preprocess)\n",
    "    embedding_df.to_pickle(EMBEDDINGS_SAVE_DIR + config[\"save_file_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = load_resnet18_model()\n",
    "test_df = pictures_df.sample(200)\n",
    "output_df = get_df_embeddings(test_df, test_model, preprocess)\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    200.000000\n",
       "mean       2.192919\n",
       "std        0.166880\n",
       "min        0.000000\n",
       "25%        2.164817\n",
       "50%        2.197933\n",
       "75%        2.233671\n",
       "max        2.422177\n",
       "Name: distances_to_first, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>owner</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>distances_to_first</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6022</th>\n",
       "      <td>picture.55d40dd7b3a541abbb94d7e2e24338ef</td>\n",
       "      <td>outfit.570f138cb9aa4b81a76b56190c107bac</td>\n",
       "      <td>[tensor(0.5149), tensor(0.4810), tensor(0.7695...</td>\n",
       "      <td>2.311112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10937</th>\n",
       "      <td>picture.9c71e18b685842938e86d3d156ca29d7</td>\n",
       "      <td>outfit.a743bd93e1a86501</td>\n",
       "      <td>[tensor(0.5660), tensor(0.4256), tensor(0.7989...</td>\n",
       "      <td>2.204357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3901</th>\n",
       "      <td>picture.3768419180bb4674879e8fbce34826c2</td>\n",
       "      <td>outfit.d7f1bc7197b6469db1b9c4faf4071cb7</td>\n",
       "      <td>[tensor(0.5320), tensor(0.4571), tensor(0.8046...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17576</th>\n",
       "      <td>picture.f8be256209814145b728193d964f2a0b</td>\n",
       "      <td>outfit.d720fa6c17bf4c688054cbda17336d6c</td>\n",
       "      <td>[tensor(0.5729), tensor(0.4188), tensor(0.7448...</td>\n",
       "      <td>2.197646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12076</th>\n",
       "      <td>picture.aab941d75a2c4e9daddabdcd1b19409b</td>\n",
       "      <td>outfit.c16aba693fc6409c81937fa356d4f3ee</td>\n",
       "      <td>[tensor(0.5748), tensor(0.4459), tensor(0.7536...</td>\n",
       "      <td>2.170306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5917</th>\n",
       "      <td>picture.548e2396d6324ec98253addbf7de9773</td>\n",
       "      <td>outfit.8dd4f2aae17b4a83817027414ba7d14f</td>\n",
       "      <td>[tensor(0.5510), tensor(0.4328), tensor(0.7743...</td>\n",
       "      <td>2.171227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>picture.05d91fac40b745ebb2af0fe67f8a8e75</td>\n",
       "      <td>outfit.c5805456353a4ee9bd110ccf3442e753</td>\n",
       "      <td>[tensor(0.4767), tensor(0.4310), tensor(0.8078...</td>\n",
       "      <td>2.248994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4555</th>\n",
       "      <td>picture.40f584a646784183b67d9e2033f0f16d</td>\n",
       "      <td>outfit.ce1df3bc87204493b2e4bd0b0973a576</td>\n",
       "      <td>[tensor(0.5514), tensor(0.4343), tensor(0.7182...</td>\n",
       "      <td>2.164786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>picture.10b38e93cc3c442ea23dc11200771237</td>\n",
       "      <td>outfit.9481d2d7251be70f</td>\n",
       "      <td>[tensor(0.5763), tensor(0.4468), tensor(0.7194...</td>\n",
       "      <td>2.265918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7341</th>\n",
       "      <td>picture.698fee413fe9415a8f54251e2a6f78dd</td>\n",
       "      <td>outfit.f6091ea8a88b4e52a4bfc6cccabe2ccb</td>\n",
       "      <td>[tensor(0.4893), tensor(0.4338), tensor(0.8133...</td>\n",
       "      <td>2.180879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             id  \\\n",
       "6022   picture.55d40dd7b3a541abbb94d7e2e24338ef   \n",
       "10937  picture.9c71e18b685842938e86d3d156ca29d7   \n",
       "3901   picture.3768419180bb4674879e8fbce34826c2   \n",
       "17576  picture.f8be256209814145b728193d964f2a0b   \n",
       "12076  picture.aab941d75a2c4e9daddabdcd1b19409b   \n",
       "...                                         ...   \n",
       "5917   picture.548e2396d6324ec98253addbf7de9773   \n",
       "428    picture.05d91fac40b745ebb2af0fe67f8a8e75   \n",
       "4555   picture.40f584a646784183b67d9e2033f0f16d   \n",
       "1194   picture.10b38e93cc3c442ea23dc11200771237   \n",
       "7341   picture.698fee413fe9415a8f54251e2a6f78dd   \n",
       "\n",
       "                                         owner  \\\n",
       "6022   outfit.570f138cb9aa4b81a76b56190c107bac   \n",
       "10937                  outfit.a743bd93e1a86501   \n",
       "3901   outfit.d7f1bc7197b6469db1b9c4faf4071cb7   \n",
       "17576  outfit.d720fa6c17bf4c688054cbda17336d6c   \n",
       "12076  outfit.c16aba693fc6409c81937fa356d4f3ee   \n",
       "...                                        ...   \n",
       "5917   outfit.8dd4f2aae17b4a83817027414ba7d14f   \n",
       "428    outfit.c5805456353a4ee9bd110ccf3442e753   \n",
       "4555   outfit.ce1df3bc87204493b2e4bd0b0973a576   \n",
       "1194                   outfit.9481d2d7251be70f   \n",
       "7341   outfit.f6091ea8a88b4e52a4bfc6cccabe2ccb   \n",
       "\n",
       "                                              embeddings  distances_to_first  \n",
       "6022   [tensor(0.5149), tensor(0.4810), tensor(0.7695...            2.311112  \n",
       "10937  [tensor(0.5660), tensor(0.4256), tensor(0.7989...            2.204357  \n",
       "3901   [tensor(0.5320), tensor(0.4571), tensor(0.8046...            0.000000  \n",
       "17576  [tensor(0.5729), tensor(0.4188), tensor(0.7448...            2.197646  \n",
       "12076  [tensor(0.5748), tensor(0.4459), tensor(0.7536...            2.170306  \n",
       "...                                                  ...                 ...  \n",
       "5917   [tensor(0.5510), tensor(0.4328), tensor(0.7743...            2.171227  \n",
       "428    [tensor(0.4767), tensor(0.4310), tensor(0.8078...            2.248994  \n",
       "4555   [tensor(0.5514), tensor(0.4343), tensor(0.7182...            2.164786  \n",
       "1194   [tensor(0.5763), tensor(0.4468), tensor(0.7194...            2.265918  \n",
       "7341   [tensor(0.4893), tensor(0.4338), tensor(0.8133...            2.180879  \n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.dist(output_df.iloc[0][\"embeddings\"], output_df.iloc[90][\"embeddings\"])\n",
    "start_point = output_df[\"embeddings\"].iloc[2]\n",
    "output_df[\"distances_to_first\"] = output_df[\"embeddings\"].apply(lambda x: torch.dist(start_point, x).item())\n",
    "display(output_df[\"distances_to_first\"].describe())\n",
    "output_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3781f7017ca0b9de91b3fa2496fb023ed6cbd54c6077891b434d73e407680117"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
